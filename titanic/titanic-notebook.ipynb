{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Preparations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# If on kaggle, copy the contents of the \"../input/\" directory to the cwd of kaggle\n","def copy_if_kaggle():\n","    import os\n","    import shutil\n","\n","    if os.getcwd() == \"/kaggle/working\":\n","        # Set the source directory\n","        src_dir = os.path.join(\"../input\", os.listdir(\"../input\")[0])\n","        # Copy all files from the source directory to the current directory\n","        for file_name in os.listdir(src_dir):\n","            shutil.copy(os.path.join(src_dir, file_name), \".\")\n","\n","\n","copy_if_kaggle()\n","# You can write up to 20GB to the cwd (/kaggle/working) that gets preserved as output when you push a version\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import libraries\n","from sklearn.model_selection import train_test_split\n","from sklearn.impute import KNNImputer, SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.feature_selection import mutual_info_classif\n","from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from sklearn.pipeline import Pipeline, make_pipeline\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.decomposition import PCA\n","from sklearn.inspection import permutation_importance\n","from sklearn.feature_selection import SelectPercentile, SelectKBest\n","from sklearn.svm import LinearSVC\n","from sklearn.compose import ColumnTransformer, make_column_selector\n","from sklearn.preprocessing import SplineTransformer\n","from sklearn import set_config\n","set_config(transform_output=\"pandas\")\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mplticker\n","%matplotlib inline\n","import seaborn as sns\n","import plotly.io as pio\n","pio.templates.default = \"plotly_dark\"\n","from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n","from scipy.spatial.distance import squareform\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","import optuna\n","# Disable trail logging\n","# optuna.logging.set_verbosity(optuna.logging.WARNING)\n","import re\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","import os\n","os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n","\n","# %%capture --no-stdout --no-display\n","# warnings.simplefilter(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot styling\n","def my_dark_style():\n","    from cycler import cycler\n","\n","    plt.style.use(\"default\")\n","    plt.style.use([\"dark_background\", \"bmh\"])\n","    plt.rcParams[\"axes.facecolor\"] = \"#23272e\"\n","    plt.rcParams[\"figure.facecolor\"] = \"#23272e\"\n","    plt.rcParams[\"axes.prop_cycle\"] = cycler(\n","        \"color\",\n","        [\n","            \"#1c90d4\",\n","            \"#ad0026\",\n","            \"#530fff\",\n","            \"#429900\",\n","            \"#d55e00\",\n","            \"#ff47ac\",\n","            \"#42baff\",\n","            \"#009e73\",\n","            \"#fff133\",\n","            \"#0072b2\",\n","        ],\n","    )\n","    # plt.rcParams['figure.figsize'] = 9, 7\n","    plt.rcParams[\"figure.autolayout\"] = True\n","\n","\n","box_kws = dict(\n","    boxprops={\"edgecolor\": \"#b2b2b2\"},\n","    capprops={\"color\": \"#b2b2b2\"},\n","    flierprops={\"markeredgecolor\": \"#b2b2b2\"},\n","    medianprops={\"color\": \"#b2b2b2\"},\n","    whiskerprops={\"color\": \"#b2b2b2\"},\n",")\n","# # plt.rcParams['boxplot.boxprops.edgecolor'] = '#b2b2b2' no such rcParam\n","# plt.rcParams['boxplot.capprops.color'] = '#b2b2b2'\n","# plt.rcParams['boxplot.flierprops.markeredgecolor'] = '#b2b2b2'\n","# plt.rcParams['boxplot.medianprops.color'] = '#b2b2b2'\n","# plt.rcParams['boxplot.whiskerprops.color'] = '#b2b2b2'\n","\n","my_dark_style()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load dataset\n","train_data = pd.read_csv(\"train.csv\")\n","test_data = pd.read_csv(\"test.csv\")\n","\n","\n","def joint_data():\n","    return pd.concat([train_data, test_data])\n","\n","\n","# Store our passenger ID for easy access\n","PassengerId = test_data.pop(\"PassengerId\")\n","train_data.drop(columns=\"PassengerId\", inplace=True)\n","\n","# Initializing random seed (integer) and/or state (instance)\n","# pass seed to CV splitters (KFold, RepeatedStratifiedKFold, etc.)\n","seed = 42\n","# pass rng to estimators and everything else;\n","# initialize a new rng for each estimator in order to prevent them from influencing each other by consuming the RNG\n","# rng = np.random.RandomState(seed)\n","# rng = np.random.default_rng(seed) # new numpy random Generator, not currently supported by sklearn\n","\n","# If an integer is passed, calling fit or split multiple times always yields the same results.\n","# If a RandomState instance is passed: fit and split will yield different results each time they are called, and the succession of calls explores all sources of entropy."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Overview"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Nominal: `Survived, Sex, Embarked, Ticket`  \n","Ordinal: `Pclass`  \n","Continuous: `Age, Fare`  \n","Discrete: `SibSp, Parch` \n","\n","`Survived` - 0 = No, 1 = Yes  \n","`Pclass` is the ticket class - 1 = 1st, 2 = 2nd, 3 = 3rd  \n","`SibSp` is the number of siblings / the number of spouses aboard the Titanic  \n","`Parch` is the number of parents / the number of children aboard the Titanic  \n","`Embarked` is the port of embarkation\t- C = Cherbourg, Q = Queenstown, S = Southampton  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(train_data.info())\n","train_data.sample(3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(test_data.info())\n","test_data.sample(3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.describe(include=\"all\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Exploratory Data Analysis and Data Cleaning"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Basic visualizations"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Passengers with first-class tickets had the highest survival rate, while those in third class had the lowest survival rate."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(train_data, x=\"Pclass\", hue=\"Survived\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[" The survival rate of females was significantly higher than that of men."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(train_data, x=\"Sex\", hue=\"Survived\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[" Passengers with no siblings/spouses/parents/children on board with them had a lower survival rate than those with a few companions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_, axs = plt.subplots(1, 2, figsize=(12, 5))\n","for ax, col in enumerate([\"SibSp\", \"Parch\"]):\n","    sns.countplot(train_data, x=col, hue=\"Survived\", ax=axs[ax])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Those who embarked form the Southampton port had the lowest survival rate. Considering the numbers and order of embarkation (S->C->Q), it is also reasonable to assume that the majority of 3rd class passengers embarked form the Southampton port."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(train_data, x=\"Embarked\", hue=\"Survived\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Children and elderly passengers had the highest survival rates, whereas those between the age of 20 and 30 had the lowest chance of surviving."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.displot(\n","    train_data, x=\"Age\", hue=\"Survived\", binwidth=10, binrange=(0, 80), kde=True\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Those with the cheapest tickets had the lowest survival rate. The distribution is strongly skewed to the right. Most tickets cost below 10, and few cost above 100."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_, axs = plt.subplots(1, 2, figsize=(12, 5))\n","sns.histplot(\n","    train_data,\n","    x=\"Fare\",\n","    hue=\"Survived\",\n","    binwidth=50,\n","    binrange=(0, 600),\n","    kde=True,\n","    ax=axs[0],\n",")\n","sns.histplot(\n","    train_data[train_data.Fare > 0],\n","    x=\"Fare\",\n","    hue=\"Survived\",\n","    log_scale=True,\n","    kde=True,\n","    ax=axs[1],\n",")\n","axs[0].set_title(\"Original\")\n","axs[1].set_title(\"Log scale\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Missing values"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are missing values in the `Age, Fare, Cabin` and `Embarked` features. All imputation of missing values is done in a manner that avoids leakage, so some steps are included in a pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.DataFrame(\n","    [train_data.isna().sum(), test_data.isna().sum()], index=[\"Train\", \"Test\"]\n",").T"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There is only one passenger with a missing `Fare` value. `Fare` is related to `Pclass`, `Embarked` and family size (`Parch` and `SibSp`) features. Median `Fare` value of a third class ticket with S as the port of embarkation for a passenger with no family is a logical choice to fill the missing value with.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_data[test_data[\"Fare\"].isna()]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_data[\"Fare\"].fillna(\n","    train_data.groupby([\"Pclass\", \"Parch\", \"SibSp\"]).Fare.median()[3][0][0],\n","    inplace=True,\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are only 2 missing values for `Embarked`. They are filled with the most frequent value."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data[\"Embarked\"].fillna(train_data.Embarked.mode()[0], inplace=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Age` missing values are imputed using the median value grouped by `Pclass` and `Sex` (as they are likely indicators of age)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AgeImputer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        sex_column = [col for col in X.columns if re.match(r\"^Sex\", col)][0]\n","        self.age_median_group = X.groupby([sex_column, \"Pclass\"])[\"Age\"].median()\n","        return self\n","\n","    def transform(self, X, y=None):\n","        sex_column = [col for col in X.columns if re.match(r\"^Sex\", col)][0]\n","        X = X.copy()\n","        X[\"Age\"] = X.apply(\n","            lambda x: self.age_median_group[x[sex_column], x[\"Pclass\"]]\n","            if pd.isna(x[\"Age\"])\n","            else x[\"Age\"],\n","            axis=1,\n","        )\n","        return X\n","\n","\n","# age_imputer is later included in the pipeline\n","age_imputer = AgeImputer()\n","sns.catplot(data=train_data, x=\"Pclass\", y=\"Age\", hue=\"Sex\", kind=\"box\", **box_kws)\n","plt.show()\n","age_imputer.fit(train_data).age_median_group"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are far too many missing `Cabin` values to meaningfully impute them, however the feature can't be ignored because some of the cabins might have higher survival rates. Another point of interest is that some passengers had multiple cabins (all on the same deck). Those with several decks paid a significantly higher fare price. Some cabin codes are preceded by an F. It is difficult to determine what this means, but judging by the price, the F is not a separate cabin, so it is removed and treated as a single cabin due to the low number of records containing it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["joint_data()[\n","    joint_data()[\"Cabin\"].map(lambda x: len(x.split()), na_action=\"ignore\") > 1\n","].head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove F_ from cabin codes\n","for data in [train_data, test_data]:\n","    data.loc[:, \"Cabin\"] = data[\"Cabin\"].map(\n","        (lambda x: x.replace(\"F \", \"\")), na_action=\"ignore\"\n","    )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["An extra feature `NCabins` is created which contains the cabin count per passenger."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data.loc[:, \"NCabins\"] = data[\"Cabin\"].map(\n","        (lambda x: len(x.split())), na_action=\"ignore\"\n","    )\n","    data.NCabins.fillna(1, inplace=True)\n","\n","joint_data().NCabins.value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Passengers with multiple cabins have a higher survival rate compared to those with only one cabin."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.displot(train_data, x=\"NCabins\", hue=\"Survived\", multiple=\"fill\", discrete=True)\n","plt.ylabel(\"Survival Ratio\")\n","plt.gca().xaxis.set_major_locator(mplticker.MultipleLocator(1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data.loc[data.NCabins == 1, \"NCabins\"] = 0 # one cabin\n","    data.loc[data.NCabins != 0, \"NCabins\"] = 1 # multiple cabins\n","\n","joint_data().NCabins.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.displot(train_data, x=\"NCabins\", hue=\"Survived\", multiple=\"fill\", discrete=True)\n","plt.ylabel(\"Survival Ratio\")\n","plt.gca().xaxis.set_major_locator(mplticker.MultipleLocator(1))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The first letter of each cabin code corresponds to the deck level where the cabin is located. `Deck` is extracted from the `Cabin` and the missing values simply encoded as \"M\". This way the missing values can be dealt with as a separate category of the `Deck` feature by the final model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Cabin\"].fillna(\"M\", inplace=True)\n","    data[\"Cabin\"] = data.Cabin.apply(lambda x: x[0])\n","    data.rename(columns={\"Cabin\": \"Deck\"}, inplace=True)\n","\n","joint_data().Deck.value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A is the topmost deck, G is the lowest passenger deck, T is the lowest deck of a ship (where the engines and boiler rooms are).  \n","Passengers on the middle decks B through F had the highest survival rates, whereas those on the lowest passenger deck, and those whose cabin codes are missing, had the lowest survival rate. The only passenger whose cabin was on the Tank Top deck (below the Orlop Deck) did not survive."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.displot(\n","    train_data.sort_values(by=\"Deck\"), x=\"Deck\", hue=\"Survived\", multiple=\"fill\"\n",")\n","plt.ylabel(\"Survival Ratio\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.Deck.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["Considering the distribution of the deck cabins and low sample count, the feature is aggregated into 4 groups to increase the significance of each single group."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Deck\"] = data[\"Deck\"].replace([\"A\", \"B\", \"C\"], \"ABC\")\n","    data[\"Deck\"] = data[\"Deck\"].replace([\"D\", \"E\"], \"DE\")\n","    data[\"Deck\"] = data[\"Deck\"].replace([\"F\", \"G\"], \"FG\")\n","    data[\"Deck\"] = data[\"Deck\"].replace(\"T\", \"M\")\n","\n","train_data[\"Deck\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.displot(\n","    train_data.sort_values(by=\"Deck\"), x=\"Deck\", hue=\"Survived\", multiple=\"fill\"\n",")\n","plt.ylabel(\"Survival Ratio\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["All missing values have been dealt with."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Feature Engineering"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Family_Size` is created by adding `SibSp, Parch` and 1, and then aggregated into 4 groups to deal with the high cardinality of the feature in the context of a small dataset. Those who travel entirely alone don't seem to have had the highest chance of survival."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Family_Size\"] = data.SibSp + data.Parch + 1\n","\n","sns.countplot(train_data, x=\"Family_Size\", hue=\"Survived\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mapping = {\n","    1: 0,  # \"Alone\"\n","    2: 1,  # \"Small\"\n","    3: 1,  # \"Small\"\n","    4: 1,  # \"Small\"\n","    5: 2,  # \"Medium\"\n","    6: 2,  # \"Medium\"\n","    7: 3,  # \"Large\"\n","    8: 3,  # \"Large\"\n","    11: 3,  # \"Large\"\n","}\n","for data in [train_data, test_data]:\n","    data.Family_Size.replace(mapping, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(train_data, x=\"Family_Size\", hue=\"Survived\")\n","plt.xticks([0, 1, 2, 3], [\"Alone\", \"Small\", \"Medium\", \"Large\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Ticket_Freq` is created by encoding the frequency with which a ticket occurs, which gives an idea of the size of the group in which one was traveling."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data[\"Ticket_Freq\"] = (\n","    joint_data().groupby(\"Ticket\")[\"Ticket\"].transform(\"count\")[:891]\n",")\n","test_data[\"Ticket_Freq\"] = (\n","    joint_data().groupby(\"Ticket\")[\"Ticket\"].transform(\"count\")[891:]\n",")\n","\n","sns.countplot(train_data, x=\"Ticket_Freq\", hue=\"Survived\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Title` is extracted from the name. All female titles are joined as \"Ms\", and Male titles with less than 10 samples are joined into a separate category \"Other\". Those with the title Mr. had the lowest survival rate. Ms. had the highest survival rate. It is noteworthy that those with the \"Master\" title, though male, also had a high survival rate."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Title\"] = (\n","        data[\"Name\"].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n","    )\n","train_data.groupby(\"Sex\").Title.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mapping = {\n","    \"Capt\": \"Other\",\n","    \"Col\": \"Other\",\n","    \"Don\": \"Other\",\n","    \"Dona\": \"Ms\",\n","    \"Dr\": \"Other\",\n","    \"Jonkheer\": \"Other\",\n","    \"Lady\": \"Ms\",\n","    \"Major\": \"Other\",\n","    \"Miss\": \"Ms\",\n","    \"Mlle\": \"Ms\",\n","    \"Mme\": \"Ms\",\n","    \"Mrs\": \"Ms\",\n","    \"Ms\": \"Ms\",\n","    \"Rev\": \"Other\",\n","    \"Sir\": \"Other\",\n","    \"the Countess\": \"Ms\",\n","}\n","for data in [train_data, test_data]:\n","    data.Title.replace(mapping, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(train_data, x=\"Title\", hue=\"Survived\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Name` and `Ticket` are dropped as they are no longer needed and contain no useful information."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data.drop(columns=[\"Name\", \"Ticket\"], inplace=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Fare` is transformed to log scale to deal with right skewness."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Log_Fare\"] = np.log1p(data[\"Fare\"])\n","\n","_, axs = plt.subplots(1, 2, figsize=(12, 5))\n","sns.histplot(\n","    train_data,\n","    x=\"Fare\",\n","    hue=\"Survived\",\n","    binwidth=50,\n","    binrange=(0, 600),\n","    kde=True,\n","    ax=axs[0],\n",")\n","sns.histplot(\n","    train_data,\n","    x=\"Log_Fare\",\n","    hue=\"Survived\",\n","    kde=True,\n","    ax=axs[1],\n",")\n","axs[0].set_title(\"Original\")\n","axs[1].set_title(\"Transformed to Log scale\")\n","\n","for data in [train_data, test_data]:\n","    data.drop(columns=\"Fare\", inplace=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The ratio between the price and quantity could prove to be a useful feature, so it is created by dividing the `Log_Fare` by `Ticket_Freq`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.catplot(train_data, x=\"Ticket_Freq\", y=\"Log_Fare\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Log_Fare/Ticket_Freq\"] = data[\"Log_Fare\"] / data[\"Ticket_Freq\"]\n","\n","sns.histplot(\n","    train_data,\n","    x=\"Log_Fare/Ticket_Freq\",\n","    hue=\"Survived\",\n","    kde=True,\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Encoding"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Nominal: `Survived, Sex, Deck, Embarked, NCabins, Title`  \n","Ordinal: `Pclass, Family_Size`  \n","Continuous: `Age, Log_Fare, Log_Fare/Ticket_Freq`  \n","Discrete: `SibSp, Parch, Ticket_Freq` "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Might come in handy\n","nominal = [\"Survived\", \"Sex\", \"Deck\", \"Embarked\", \"NCabins\", \"Title\"]\n","ordinal = [\"Pclass\", \"Family_Size\"]\n","continuous = [\"Age\", \"Log_Fare\", \"Log_Fare/Ticket_Freq\"]\n","discrete = [\"SibSp\", \"Parch\", \"Ticket_Freq\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["joint_data().head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["All features except `Sex, Deck, Embarked` and `Title` are already properly encoded, so only they are encoded (as a one-hot numeric array)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feat = [\"Sex\", \"Deck\", \"Embarked\", \"Title\"]\n","ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False)\n","train_data = train_data.drop(columns=feat).join(ohe.fit_transform(train_data[feat]))\n","test_data = test_data.drop(columns=feat).join(ohe.transform(test_data[feat]))\n","\n","joint_data().info()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Dealing with Multicollinearity"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define transformer class for filtering out highly correlated features and plotting correlation\n","# turns out this isn't very computationally efficient.. tuning hyperparameters takes too long\n","class SelectCorrelationClusters(BaseEstimator, TransformerMixin):\n","    def __init__(\n","        self,\n","        threshold=0.7,\n","        corr_method=\"spearman\",\n","        linkage_method=\"ward\",\n","        optimal_ordering=True,\n","    ):\n","        self.threshold = threshold\n","        self.corr_method = corr_method\n","        self.linkage_method = linkage_method\n","        self.optimal_ordering = optimal_ordering\n","\n","    def fit(self, X, y=None):\n","        X = pd.DataFrame(X)\n","        # Remove constant columns and calculate correlation\n","        self.constant_cols = np.where(X.nunique() == 1)[0]\n","        self.corr_ = X.drop(columns=X.columns[self.constant_cols]).corr(\n","            method=self.corr_method\n","        )\n","        # Convert the correlation matrix to a distance matrix before performing hierarchical clustering\n","        distance_matrix = 1 - self.corr_.abs()\n","        self.Z = linkage(\n","            squareform(distance_matrix, checks=False),\n","            \"ward\",\n","            optimal_ordering=self.optimal_ordering,\n","        )\n","        # Extract the cluster labels for each feature based on the specified threshold\n","        clusters = fcluster(self.Z, self.threshold, criterion=\"distance\")\n","        # Filter out highly correlated features\n","        # Keep one feature per cluster with the maximum average correlation\n","        self.kept_features = []\n","        for cluster_id in set(clusters):\n","            clusters_boolean = clusters == cluster_id\n","            corr_vals = self.corr_.iloc[clusters_boolean, clusters_boolean]\n","            best_feature_idx = corr_vals.abs().sum(axis=1).idxmax()\n","            self.kept_features.append(corr_vals.columns.get_loc(best_feature_idx))\n","        return self\n","\n","    def transform(self, X, y=None):\n","        X = pd.DataFrame(X)\n","        return X.drop(columns=X.columns[self.constant_cols]).iloc[:, self.kept_features]\n","\n","    def plot(self, annot=True, cmap=\"RdBu\", split=True):\n","        if not split:\n","            # Plot clustermap\n","            sns.clustermap(\n","                self.corr_.round(2),\n","                row_linkage=self.Z,\n","                col_linkage=self.Z,\n","                cmap=cmap,\n","                annot=annot,\n","                annot_kws={\"size\": 8},\n","                vmin=-1,\n","                vmax=1,\n","                figsize=(15, 12),\n","                dendrogram_ratio=0.2,\n","            )\n","        else:\n","            # Plot dendrogram with correlation heatmap\n","            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 8))\n","            dendro = dendrogram(\n","                self.Z,\n","                labels=self.corr_.columns,\n","                ax=ax1,\n","                leaf_rotation=90,\n","            )\n","            order = dendro[\"leaves\"]\n","            sns.heatmap(\n","                self.corr_.iloc[order, order],\n","                cmap=cmap,\n","                annot=annot,\n","                vmin=-1,\n","                vmax=1,\n","                # linewidths=0.01,\n","                # linecolor=\"#23272e\",\n","                ax=ax2,\n","            )\n","            fig.tight_layout()\n","        plt.show()\n","\n","\n","def calculate_vif(X):\n","    # Add constant column\n","    # X = add_constant(X)\n","    X = X.assign(const=1)\n","\n","    # Calculate VIF for each variable\n","    vif = pd.DataFrame()\n","    vif[\"Variable\"] = X.columns\n","    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","    return vif"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["When a dummy variable that represents more than two categories has a high VIF score, multicollinearity does not necessarily exist. The variables will always have high VIFs if there is a small portion of cases in the category, regardless of whether the categorical variables are correlated to other variables, so dummy encoded nominal variables are ignored.  \n","`Family_Size, SibSp` and `Parch` have an infinite VIF score and are perfectly multicollinear. They all have high correlation scores, along with `Ticket_Freq`. Considering their MI scores, `Family_Size` is kept and the rest are dropped, along with `Log_Fare`, which also has high correlation scores with those previously mentioned and with `Log_Fare/Ticket_Freq`, with the exception that it also has high correlation with `Pclass` (which the rest do not), which results in a higher VIF score than `Log_Fare/Ticket_Freq`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# impd_age is needed to demonstrate how things work with imputed age values,\n","# which will actually be imputed later, during CV\n","impd_age = age_imputer.fit_transform(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["display(calculate_vif(impd_age[discrete + continuous + ordinal]))\n","SelectCorrelationClusters().fit(impd_age[discrete + continuous + ordinal]).plot()\n","mutinf = pd.Series(\n","    mutual_info_classif(\n","        impd_age.drop(columns=\"Survived\"),\n","        impd_age[\"Survived\"],\n","        discrete_features=[\n","            True\n","            if col\n","            not in [\n","                \"Age\",\n","                \"Log_Fare\",\n","                \"Log_Fare/Ticket_Freq\",\n","            ]\n","            else False\n","            for col in impd_age.drop(columns=\"Survived\").columns\n","        ],\n","        random_state=seed,\n","    ),\n","    index=impd_age.drop(columns=\"Survived\").columns,\n",").sort_values(ascending=False)\n","sns.barplot(x=mutinf, y=mutinf.index.astype(str))\n","plt.xlabel(\"Mutual Information score\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data.drop(\n","        columns=[\n","            \"SibSp\",\n","            \"Parch\",\n","            \"Ticket_Freq\",\n","            \"Log_Fare\",\n","        ],\n","        inplace=True,\n","    )\n","\n","# Might come in handy\n","nominal = [\"Survived\", \"Sex\", \"Deck\", \"Embarked\", \"NCabins\", \"Title\"]\n","ordinal = [\"Pclass\", \"Family_Size\"]\n","continuous = [\"Age\", \"Log_Fare/Ticket_Freq\"]\n","discrete = []"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now all VIF scores are within reasonable bounds."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["calculate_vif(impd_age[discrete + continuous + ordinal])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Reciprocal"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next the multiplicative inverse of ordinal/continuous/discrete features is added."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# class ReciprocalTransformer(BaseEstimator, TransformerMixin):\n","#     def get_thresholds(self, column):\n","#         # Find the minimum positive non-zero value (value closest to zero) in the column\n","#         min_nonzero = np.min(column[column != 0].abs())\n","#         # Calculate the closest 10^n value below the minimum non-zero value in the column\n","#         self.threshold[column.name] = 10 ** np.floor(np.log10(min_nonzero))\n","\n","#     def transform_zeros(self, column):\n","#         column_values = column.copy()\n","#         # Find zero values in the column and replace them with the threshold value\n","#         column_values.loc[column_values == 0] = self.threshold[column_values.name]\n","#         return column_values\n","\n","#     def fit(self, X, y=None):\n","#         # Store thresholds by column name\n","#         self.threshold = {}\n","#         # Exclude columns that are OHE dummies\n","#         self.selected_columns = X.loc[\n","#             :, ~np.all(np.logical_or(X == 0, X == 1), axis=0)\n","#         ].columns\n","#         # Get thresholds\n","#         X[self.selected_columns].apply(self.get_thresholds)\n","#         return self\n","\n","#     def transform(self, X, y=None):\n","#         # Apply the transform_zeros function to each selected column\n","#         transformed_columns = X[self.selected_columns].apply(self.transform_zeros)\n","#         # Take the reciprocal of the transformed columns\n","#         reciprocals = 1 / transformed_columns\n","#         return X.join(reciprocals, rsuffix=\"_inverse\")\n","\n","\n","# ReciprocalTransformer().fit(train_data).transform(train_data).filter(\n","#     regex=\"_inverse$\"\n","# ).head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Model training and selection"]},{"cell_type":"markdown","metadata":{},"source":["The data is prepared along with a function for calculating mutual information between the dependent and independent variables for feature selection."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prepare data\n","X = train_data.copy()\n","y = X.pop(\"Survived\")\n","\n","# Define score function to go into SelectKBest\n","discrete_features = [\n","    True\n","    if col\n","    not in [\n","        \"Age\",\n","        \"Log_Fare/Ticket_Freq\",\n","    ]\n","    else False\n","    for col in X.columns\n","]\n","\n","\n","def mut_info(X, y):\n","    return mutual_info_classif(\n","        X,\n","        y,\n","        discrete_features=discrete_features,\n","        random_state=seed,\n","    )\n","\n","\n","# # Define transformer to clean up after adding feature interactions and standardizing\n","# class RemoveDuplicates(BaseEstimator, TransformerMixin):\n","#     def fit(self, X, y=None):\n","#         df = pd.DataFrame(X)\n","#         self.not_dup = list(df.columns[~df.T.duplicated()])\n","#         return self\n","\n","#     def transform(self, X):\n","#         df = pd.DataFrame(X)\n","#         return df.loc[:, self.not_dup]  # .to_numpy()\n","\n","\n","# # Define function to create a DataFrame from top scores\n","# def get_top_scores(study, score_count=7):\n","#     df = (\n","#         study.trials_dataframe(attrs=(\"value\", \"duration\", \"params\"))\n","#         .sort_values(by=\"value\", ascending=False)\n","#         .head(score_count)\n","#     ).rename(\n","#         columns=lambda x: re.sub(r\"^params_\", \"\", x)\n","#     )  # remove params_ from col names\n","#     df[\"duration\"] = df[\"duration\"].dt.total_seconds()\n","#     return df"]},{"cell_type":"markdown","metadata":{},"source":["Pipeline steps are initialized and passed to the `cv_pipe` function which takes an optuna `trial` and the `model` to by optimized and returns a score used by optuna for hyperparameter optimization."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize pipeline steps\n","age_imputer = AgeImputer()\n","standardize = StandardScaler()\n","to_spline = make_column_selector(pattern=r\"Age|Log_Fare\\/Ticket_Freq\")\n","splines = ColumnTransformer(\n","    [\n","        (\n","            \"splines\",\n","            SplineTransformer(\n","                n_knots=3,\n","                degree=3,\n","                extrapolation=\"linear\",\n","                knots=\"quantile\",\n","            ),\n","            to_spline,\n","        )\n","    ],\n","    remainder=\"passthrough\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the objective of the study without the model\n","def cv_pipe(trial, model, X, y):\n","    # Create Pipeline\n","    pipe = make_pipeline(\n","        age_imputer,\n","        SelectKBest(mut_info, k=trial.suggest_int(\"k_best\", 7, 17)),\n","        splines,\n","        standardize,\n","        model,\n","    )\n","\n","    cv_score = cross_val_score(\n","        pipe,\n","        X,\n","        y,\n","        cv=RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=seed),\n","        n_jobs=-1,\n","    ).mean()\n","    return cv_score"]},{"cell_type":"markdown","metadata":{},"source":["A function is defined which takes an optuna compatible objective function and performs nested cross-validation for the given model, populating the global `model_results` DataFrame. The procedure involves treating model hyperparameter optimization as part of the model itself and evaluating it within the broader k-fold cross-validation procedure for evaluating models for comparison and selection. As such, the k-fold cross-validation procedure for model hyperparameter optimization is nested inside the k-fold cross-validation procedure for model selection."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to perform nested cross-validation of objective function using optuna for hyperparameter optimization\n","def perform_nested_cv(\n","    objective,\n","    X,\n","    y,\n","    n_trials=300,\n","    study_timeout=700,\n","    n_splits=4,\n","    n_repeats=1,\n","    random_state=seed,\n","):\n","    print(\"Inner loop:\\n\")\n","    # Outer cross-validation\n","    cv_scores = []\n","    outer_scores = []\n","    outer_cv = RepeatedStratifiedKFold(\n","        n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n","    )\n","    split_n = 1\n","    for i, (train_index, test_index) in enumerate(outer_cv.split(X, y), 1):\n","        X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n","        y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n","\n","        # Define unique identifier of the study\n","        study_name = f\"{model.__class__.__name__}_split_{i}\"\n","        study = optuna.create_study(\n","            storage=storage_name,\n","            study_name=study_name,\n","            sampler=optuna.samplers.TPESampler(\n","                multivariate=True, seed=seed, warn_independent_sampling=False\n","            ),\n","            direction=\"maximize\",\n","            load_if_exists=True,\n","        )\n","\n","        # Perform hyperparameter optimization (with inner cross-validation)\n","        study.optimize(\n","            lambda trial: objective(trial, X_train_outer, y_train_outer),\n","            n_trials=n_trials,\n","            timeout=study_timeout,\n","            catch=(ValueError),\n","            # callbacks=[logging_callback],\n","        )\n","        cv_scores.append(study.best_value)\n","        print(f\"    {study_name}\")\n","        split_n += 1\n","\n","        # Select and assign best hyperparameters\n","        best_study_params_ = study.best_params\n","        print(f\"    Best params: {best_study_params_}\")\n","        k_best = best_study_params_.pop(\"k_best\")\n","        # Assign model parameters to estimator\n","        model.set_params(**best_study_params_)\n","        # Create pipeline\n","        pipe = make_pipeline(\n","            age_imputer,\n","            SelectKBest(mut_info, k=k_best),\n","            splines,\n","            standardize,\n","            model,\n","        )\n","\n","        # Refit the model with best hyperparameters on entire outer training data\n","        pipe.fit(X_train_outer, y_train_outer)\n","\n","        # Evaluate on the outer test set\n","        score = pipe.score(X_test_outer, y_test_outer)\n","        outer_scores.append(score)\n","        print(\n","            f\"    inner CV score: {study.best_value:.3f} | outer test score: {score:.3f}\\n\"\n","        )\n","    nest_mean, nest_std, inner_mean, inner_std = (\n","        np.mean(outer_scores),\n","        np.std(outer_scores),\n","        np.mean(cv_scores),\n","        np.std(cv_scores),\n","    )\n","    model_results.loc[model.__class__.__name__] = [\n","        nest_mean,\n","        nest_std,\n","        inner_mean,\n","        inner_std,\n","    ]\n","    print(\n","        f\"Outer loop:\\n\\n    mean outer test score: {np.mean(outer_scores):.3f} +/- {np.std(outer_scores):.3f}\"\n","    )\n","    # return outer_scores"]},{"cell_type":"markdown","metadata":{},"source":["Preparations are made to run the nested cross-validations and store the results."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define DB file name for storing tuning results\n","db_name = \"nested_titanic_study\"\n","storage_name = f\"sqlite:///{db_name}.db\"\n","\n","# Create DataFrame to store results of model evaluations\n","model_results = pd.DataFrame(\n","    columns=[\n","        \"mean_nested_CV\",\n","        \"std_nested_CV\",\n","        \"mean_inner_CV\",\n","        \"std_inner_CV\",\n","    ]\n",")\n","model_results.index.name = \"Algorithm\"\n","\n","# Turn off optuna log notes.\n","optuna.logging.set_verbosity(optuna.logging.WARN)\n","\n","\n","# Define a function to output a log only when the best value is updated\n","def logging_callback(study, frozen_trial):\n","    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n","    if previous_best_value != study.best_value:\n","        study.set_user_attr(\"previous_best_value\", study.best_value)\n","        print(\n","            f\"Trial {frozen_trial.number} finished with best value: {frozen_trial.value} and parameters: {frozen_trial.params}.\\n\"\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["Nested cross-validation is performed for a number of classification algorithms."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### K-Nearest Neighbours"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = KNeighborsClassifier(\n","    n_jobs=-1,\n",")\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial, X, y):\n","    # Set estimator parameters\n","    model.set_params(n_neighbors=trial.suggest_int(\"n_neighbors\", 4, 40))\n","    return cv_pipe(trial, model, X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform nested cross-validation\n","perform_nested_cv(objective_w_model, X, y)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = RandomForestClassifier(n_jobs=-1, random_state=rng)\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial, X, y):\n","    # Set estimator parameters\n","    model.set_params(\n","        max_depth=trial.suggest_int(\"max_depth\", 5, 21),\n","        min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 5, 45),\n","        max_features=trial.suggest_float(\"max_features\", 0.1, 0.5),\n","        ccp_alpha=trial.suggest_float(\"ccp_alpha\", 1e-5, 0.3),\n","    )\n","    return cv_pipe(trial, model, X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform nested cross-validation\n","perform_nested_cv(objective_w_model, X, y)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Support Vector Machine"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = SVC(gamma=\"scale\", max_iter=100000, random_state=rng, cache_size=1000)\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial, X, y):\n","    # Set estimator parameters\n","    model.set_params(\n","        C=trial.suggest_float(\"C\", 1e-4, 1000, log=True),\n","        kernel=trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"sigmoid\"]),\n","    )\n","    return cv_pipe(trial, model, X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform nested cross-validation\n","perform_nested_cv(objective_w_model, X, y)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### AdaBoost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = AdaBoostClassifier(estimator=DecisionTreeClassifier(), random_state=rng)\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial, X, y):\n","    # Set estimator parameters\n","    model.set_params(\n","        # estimator__max_depth=trial.suggest_int(\"estimator__max_depth\", 1, 3),\n","        n_estimators=trial.suggest_int(\"n_estimators\", 10, 100),\n","        learning_rate=trial.suggest_float(\"learning_rate\", 1e-7, 1, log=True),\n","    )\n","    return cv_pipe(trial, model, X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform nested cross-validation\n","perform_nested_cv(objective_w_model, X, y)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Gradient Boosting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = GradientBoostingClassifier(\n","    random_state=rng,\n",")\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial, X, y):\n","    # Set estimator parameters\n","    model.set_params(\n","        n_estimators=trial.suggest_int(\"n_estimators\", 30, 400),\n","        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1, log=True),\n","        subsample=trial.suggest_float(\"subsample\", 0.1, 1),\n","        min_samples_split=trial.suggest_float(\"min_samples_leaf\", 1e-3, 1e-1, log=True),\n","        max_depth=trial.suggest_int(\"max_depth\", 2, 30),\n","        max_features=trial.suggest_float(\"max_features\", 0.1, 0.9),\n","        ccp_alpha=trial.suggest_float(\"ccp_alpha\", 1e-9, 0.1),\n","    )\n","    return cv_pipe(trial, model, X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform nested cross-validation\n","perform_nested_cv(objective_w_model, X, y)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = XGBClassifier(booster=\"gbtree\", random_state=rng, verbosity=0)\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial, X, y):\n","    # Set estimator parameters\n","    model.set_params(\n","        n_estimators=trial.suggest_int(\"n_estimators\", 50, 300),\n","        eta=trial.suggest_float(\"eta\", 1e-3, 1, log=True),\n","        subsample=trial.suggest_float(\"subsmaple\", 0.5, 1),\n","        max_depth=trial.suggest_int(\"max_depth\", 6, 20),\n","        min_child_weight=trial.suggest_float(\"min_child_weight\", 1e-3, 10, log=True),\n","        scale_pos_weight=trial.suggest_float(\n","            \"scale_pos_weight\", 0.56, 0.69\n","        ),  # (negative instances/positive instances) +/- 10%\n","        reg_alpha=trial.suggest_float(\"reg_alpha\", 0, 10),\n","    )\n","    return cv_pipe(trial, model, X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform nested cross-validation\n","perform_nested_cv(objective_w_model, X, y)"]},{"cell_type":"markdown","metadata":{},"source":["The results of the nested cross-validation show that the RandomForestClassifier algorithm has the highest accuracy score on held out test data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_results.sort_values(by=\"mean_nested_CV\", ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Final Model"]},{"cell_type":"markdown","metadata":{},"source":["After extensive tuning, the final model resulted in an out of bag (OOB) accuracy score of ≈0.81."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = RandomForestClassifier(\n","    n_estimators=300,\n","    max_depth=21,\n","    min_samples_leaf=43,\n","    max_features=0.11,\n","    ccp_alpha=0.0001,\n","    n_jobs=-1,\n","    random_state=rng,\n","    oob_score=True,\n",")\n","# Create Pipeline\n","pipe = make_pipeline(\n","    age_imputer,\n","    SelectKBest(mut_info, k=8),\n","    splines,\n","    model,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train model and generate predictions\n","predictions = pipe.fit(X, y).predict(test_data)\n","# Create submission CSV\n","predictions_df = pd.DataFrame({\"PassengerId\": PassengerId, \"Survived\": predictions})\n","predictions_df.to_csv('submission.csv', header=True, index=False)\n","print(f\"Out of Bag (OOB) accuracy score: {model.oob_score_}\")"]},{"cell_type":"markdown","metadata":{},"source":["The `Log_Fare/Ticket_Freq` feature tuned out to be the most important among all features, followed by the title, sex and age features. The `Embarked` feature was removed entirely by the pipeline, as it didn't prove useful to the final score. Note, that the total number of features supplied to the model is actually 16, after the spline transformation of continuous features is applied."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate permutation importance scores\n","perm_pipe = permutation_importance(\n","    pipe, X, y, random_state=seed, n_repeats=10, n_jobs=-1\n",")\n","perm_imp = (\n","    pd.DataFrame(perm_pipe.importances, index=X.columns)\n","    .reset_index()\n","    .melt(id_vars=\"index\")\n",")\n","# Filter out features removed by SelectKBest\n","perm_imp = perm_imp.groupby(\"index\").filter(lambda group: group[\"value\"].mean() > 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot importance scores\n","sns.barplot(\n","    perm_imp,\n","    x=\"value\",\n","    y=\"index\",\n","    order=perm_imp.groupby(\"index\")[\"value\"].mean().sort_values(ascending=False).index,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.n_features_in_"]},{"cell_type":"markdown","metadata":{},"source":["Uncomment and run the following to open a real-time dashboard for Optuna (via your browser of choice) which contains detailed information about all trials for conveniently exploring the optimization history, hyperparameter importances, etc. in graphs and tables."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !optuna-dashboard sqlite:///nested_titanic_study.db"]},{"cell_type":"markdown","metadata":{},"source":["Or execute the above command in a terminal (without the exclamation mark), and run the following cell to view the dashboard directly from within this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %%html\n","# <iframe src=\"http://127.0.0.1:8080/\" width=\"1330\" height=\"550\"></iframe>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
