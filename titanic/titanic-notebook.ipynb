{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Preparations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# If on kaggle, copy the contents of the \"../input/\" directory to the cwd of kaggle\n","def copy_if_kaggle():\n","    import os\n","    import shutil\n","\n","    if os.getcwd() == \"/kaggle/working\":\n","        # Set the source directory\n","        src_dir = os.path.join(\"../input\", os.listdir(\"../input\")[0])\n","        # Copy all files from the source directory to the current directory\n","        for file_name in os.listdir(src_dir):\n","            shutil.copy(os.path.join(src_dir, file_name), \".\")\n","\n","\n","copy_if_kaggle()\n","# You can write up to 20GB to the cwd (/kaggle/working) that gets preserved as output when you push a version\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import libraries\n","from sklearn.model_selection import train_test_split\n","from sklearn.impute import KNNImputer, SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import mutual_info_classif\n","from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from sklearn.pipeline import Pipeline, make_pipeline\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.decomposition import PCA\n","from sklearn.inspection import permutation_importance\n","from sklearn.feature_selection import SelectPercentile\n","from sklearn.svm import LinearSVC\n","from sklearn import set_config\n","set_config(transform_output=\"pandas\")\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mplticker\n","%matplotlib inline\n","import seaborn as sns\n","import plotly.io as pio\n","pio.templates.default = \"plotly_dark\"\n","from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n","from scipy.spatial.distance import squareform\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","import optuna\n","# Disable trail logging\n","# optuna.logging.set_verbosity(optuna.logging.WARNING)\n","import re\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","import os\n","os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n","\n","# %%capture --no-stdout --no-display\n","# warnings.simplefilter(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot styling\n","def my_dark_style():\n","    from cycler import cycler\n","\n","    plt.style.use(\"default\")\n","    plt.style.use([\"dark_background\", \"bmh\"])\n","    plt.rcParams[\"axes.facecolor\"] = \"#23272e\"\n","    plt.rcParams[\"figure.facecolor\"] = \"#23272e\"\n","    plt.rcParams[\"axes.prop_cycle\"] = cycler(\n","        \"color\",\n","        [\n","            \"#1c90d4\",\n","            \"#ad0026\",\n","            \"#530fff\",\n","            \"#429900\",\n","            \"#d55e00\",\n","            \"#ff47ac\",\n","            \"#42baff\",\n","            \"#009e73\",\n","            \"#fff133\",\n","            \"#0072b2\",\n","        ],\n","    )\n","    # plt.rcParams['figure.figsize'] = 9, 7\n","    plt.rcParams[\"figure.autolayout\"] = True\n","\n","\n","box_kws = dict(\n","    boxprops={\"edgecolor\": \"#b2b2b2\"},\n","    capprops={\"color\": \"#b2b2b2\"},\n","    flierprops={\"markeredgecolor\": \"#b2b2b2\"},\n","    medianprops={\"color\": \"#b2b2b2\"},\n","    whiskerprops={\"color\": \"#b2b2b2\"},\n",")\n","# # plt.rcParams['boxplot.boxprops.edgecolor'] = '#b2b2b2' no such rcParam\n","# plt.rcParams['boxplot.capprops.color'] = '#b2b2b2'\n","# plt.rcParams['boxplot.flierprops.markeredgecolor'] = '#b2b2b2'\n","# plt.rcParams['boxplot.medianprops.color'] = '#b2b2b2'\n","# plt.rcParams['boxplot.whiskerprops.color'] = '#b2b2b2'\n","\n","my_dark_style()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load dataset\n","train_data = pd.read_csv(\"train.csv\")\n","test_data = pd.read_csv(\"test.csv\")\n","\n","\n","def joint_data():\n","    return pd.concat([train_data, test_data])\n","\n","\n","all_data = joint_data()\n","\n","\n","# Store our passenger ID for easy access\n","PassengerId = test_data.pop(\"PassengerId\")\n","train_data.drop(columns=\"PassengerId\", inplace=True)\n","\n","# Initializing random seed (integer) and/or state (instance)\n","# pass seed to CV splitters (KFold, RepeatedStratifiedKFold, etc.)\n","seed = 42\n","# pass rng to estimators and everything else;\n","# initialize a new rng for each estimator in order to prevent them from influencing each other by consuming the RNG\n","# rng = np.random.RandomState(seed)\n","# rng = np.random.default_rng(seed) # new numpy random Generator, not currently supported by sklearn\n","\n","# If an integer is passed, calling fit or split multiple times always yields the same results.\n","# If a RandomState instance is passed: fit and split will yield different results each time they are called, and the succession of calls explores all sources of entropy."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Overview"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Nominal: `Survived, Sex, Embarked, Ticket`  \n","Ordinal: `Pclass`  \n","Continuous: `Age, Fare`  \n","Discrete: `SibSp, Parch` \n","\n","`Survived` - 0 = No, 1 = Yes  \n","`Pclass` is the ticket class - 1 = 1st, 2 = 2nd, 3 = 3rd  \n","`SibSp` is the number of siblings / the number of spouses aboard the Titanic  \n","`Parch` is the number of parents / the number children aboard the Titanic  \n","`Embarked` is the port of embarkation\t- C = Cherbourg, Q = Queenstown, S = Southampton  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(train_data.info())\n","train_data.sample(3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(test_data.info())\n","test_data.sample(3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.describe(include=\"all\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exploratory Data Analysis and Data Cleaning"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Basic visualizations"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Passengers with first-class tickets had the highest survival rate, while those in third class had the lowest survival rate."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(train_data, x=\"Pclass\", hue=\"Survived\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[" The survival rate of females was significantly higher than that of men."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(train_data, x=\"Sex\", hue=\"Survived\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[" Passengers with no siblings/spouses/parents/children on board with them seem to have had a lower survival rate than those with a few companions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_, axs = plt.subplots(1, 2, figsize=(12, 5))\n","for ax, col in enumerate([\"SibSp\", \"Parch\"]):\n","    sns.countplot(train_data, x=col, hue=\"Survived\", ax=axs[ax])\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Those who embarked form the Southampton port had the lowest survival rate. Considering the numbers and order of embarkation (S->C->Q), it is also reasonable to assume that the majority of 3rd class passengers embarked form the Southampton port."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(train_data, x=\"Embarked\", hue=\"Survived\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Children and elderly passengers had the highest survival rates, whereas those between the age of 20 and 30 had the lowest chance of surviving."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.displot(\n","    train_data, x=\"Age\", hue=\"Survived\", binwidth=10, binrange=(0, 80), kde=True\n",")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Those with the cheapest tickets had the lowest survival rate. The distribution is strongly skewed to the right. Most tickets costing below 10, and few cost above 100."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_, axs = plt.subplots(1, 2, figsize=(12, 5))\n","sns.histplot(\n","    train_data,\n","    x=\"Fare\",\n","    hue=\"Survived\",\n","    binwidth=50,\n","    binrange=(0, 600),\n","    kde=True,\n","    ax=axs[0],\n",")\n","sns.histplot(\n","    train_data[train_data.Fare > 0],\n","    x=\"Fare\",\n","    hue=\"Survived\",\n","    log_scale=True,\n","    kde=True,\n","    ax=axs[1],\n",")\n","axs[0].set_title(\"Original\")\n","axs[1].set_title(\"Log scale\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Missing values"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are missing values in the `Age, Fare, Cabin` and `Embarked` features. All imputation of missing values is done in a manner that avoids leakage, so some steps are included in a pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.DataFrame(\n","    [train_data.isna().sum(), test_data.isna().sum()], index=[\"Train\", \"Test\"]\n",").T\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There is only one passenger with a missing `Fare` value. `Fare` is related to `Pclass`, `Embarked` and family size (`Parch` and `SibSp`) features. Median `Fare` value of a third class ticket with S as the port of embarkation for a passenger with no family is a logical choice to fill the missing value with.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_data[test_data[\"Fare\"].isna()]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Blame black formatter for the lousy formatting..\n","test_data[\"Fare\"].fillna(\n","    train_data.groupby([\"Pclass\", \"Embarked\", \"Parch\", \"SibSp\"]).Fare.mean()[3][\"S\"][0][\n","        0\n","    ],\n","    inplace=True,\n",")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are only 2 missing values for `Embarked`. They are filled with the most frequent value."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data[\"Embarked\"].fillna(\"S\", inplace=True)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Age` missing values are imputed using the median value grouped by `Pclass` and `Sex` (as they are likely indicators of age)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AgeImputer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        sex_column = [col for col in X.columns if re.match(r\"^Sex\", col)][0]\n","        self.age_median_group = X.groupby([sex_column, \"Pclass\"])[\"Age\"].median()\n","        return self\n","\n","    def transform(self, X, y=None):\n","        sex_column = [col for col in X.columns if re.match(r\"^Sex\", col)][0]\n","        X = X.copy()\n","        X[\"Age\"] = X.apply(\n","            lambda x: self.age_median_group[x[sex_column], x[\"Pclass\"]]\n","            if pd.isna(x[\"Age\"])\n","            else x[\"Age\"],\n","            axis=1,\n","        )\n","        return X\n","\n","\n","# age_imputer is later included in the pipeline\n","age_imputer = AgeImputer()\n","sns.catplot(data=train_data, x=\"Pclass\", y=\"Age\", hue=\"Sex\", kind=\"box\", **box_kws)\n","plt.show()\n","age_imputer.fit(train_data).age_median_group\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# `Age` missing values are imputed using the mean value from the nearest neighbors (using `Pclass` and `Sex` as features, as they are likely indicators of age)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# _, axs = plt.subplots(1, 4, figsize=(15, 5))\n","# for ax, col in enumerate([\"Pclass\", \"Sex\"]):\n","#     sns.boxplot(\n","#         train_data,\n","#         x=col,\n","#         y=\"Age\",\n","#         ax=axs[ax],\n","#         # boxprops={\"edgecolor\": \"#b2b2b2\"},\n","#         # capprops={\"color\": \"#b2b2b2\"},\n","#         # flierprops={\"markeredgecolor\": \"#b2b2b2\"},\n","#         # medianprops={\"color\": \"#b2b2b2\"},\n","#         # whiskerprops={\"color\": \"#b2b2b2\"},\n","#         **box_kws\n","#     )\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train_data[age_na := train_data[\"Age\"].isna()].head(3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# neighbors = [\"Age\", \"Pclass\", \"Sex\"]\n","# # Encode `Sex`\n","# ord_enc = OneHotEncoder(drop=\"first\", sparse_output=False)\n","# train_data[\"Sex\"] = ord_enc.fit_transform(train_data[[\"Sex\"]])\n","# test_data[\"Sex\"] = ord_enc.transform(test_data[[\"Sex\"]])\n","# # Scale\n","# ss = StandardScaler()\n","# train_data[neighbors] = ss.fit_transform(train_data[neighbors])\n","# test_data[neighbors] = ss.transform(test_data[neighbors])\n","# # Impute `Age`\n","# knn_imp = KNNImputer(n_neighbors=3, weights=\"distance\")\n","# train_data[neighbors] = knn_imp.fit_transform(train_data[neighbors])\n","# test_data[neighbors] = knn_imp.transform(test_data[neighbors])\n","# # Temporarily format back to previous scale and decode `Sex` back to male/female for interpretability\n","# for data in [train_data, test_data]:\n","#     data[neighbors] = ss.inverse_transform(data[neighbors])\n","#     data[neighbors[1:]] = data[neighbors[1:]].round().astype(int)\n","#     data[\"Sex\"] = ord_enc.inverse_transform(data[[\"Sex\"]])\n","# # CV flag\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train_data[age_na].head(3)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are far too many missing `Cabin` values to meaningfully impute them, however the feature can't be ignored because some of the cabins might have higher survival rates. Another point of interest is that some passengers had multiple cabins (all on the same deck). Those with several decks paid a significantly higher fare price. Some cabin codes are preceded by an F. It is difficult to determine what this means, but judging by the price, the F is not a separate cabin, so it is removed and treated as a single cabin due to the low number of records containing it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["joint_data()[\n","    joint_data()[\"Cabin\"].map(lambda x: len(x.split()), na_action=\"ignore\") > 1\n","].head(10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove F_ from cabin codes\n","for data in [train_data, test_data]:\n","    data.loc[:, \"Cabin\"] = data[\"Cabin\"].map(\n","        (lambda x: x.replace(\"F \", \"\")), na_action=\"ignore\"\n","    )\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["An extra feature `NCabins` is created which contains the cabin count per passenger."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data.loc[:, \"NCabins\"] = data[\"Cabin\"].map(\n","        (lambda x: len(x.split())), na_action=\"ignore\"\n","    )\n","    data.NCabins.fillna(1, inplace=True)\n","\n","joint_data().NCabins.value_counts()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Passengers with multiple cabins have a higher survival rate compared to those with only one cabin."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.displot(train_data, x=\"NCabins\", hue=\"Survived\", multiple=\"fill\", discrete=True)\n","plt.ylabel(\"Survival Ratio\")\n","plt.gca().xaxis.set_major_locator(mplticker.MultipleLocator(1))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# joint_data().groupby(\"Cabin\", as_index=False).Ticket.nunique()[\n","#     joint_data()\n","#     .groupby(\"Cabin\", as_index=False)\n","#     .Ticket.count()[\"Cabin\"]\n","#     .map(lambda x: len(x.split()), na_action=\"ignore\")\n","#     > 1\n","# ]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The first letter of each cabin code corresponds to the deck level where the cabin is located. `Deck` is extracted from the `Cabin` and the missing values simply encoded as \"M\". This way the missing values can be dealt with as a separate category of the `Deck` feature by the final model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Cabin\"].fillna(\"M\", inplace=True)\n","    data[\"Cabin\"] = data.Cabin.apply(lambda x: x[0])\n","    data.rename(columns={\"Cabin\": \"Deck\"}, inplace=True)\n","\n","joint_data().Deck.value_counts()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# XXXXXXX ordinal or nominal?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Deck` is an ordinal categorical feature. A is the topmost deck, G is the lowest passenger deck, T is the lowest deck of a ship (where the engines and boiler rooms are).  \n","Passengers on the middle decks B through F had the highest survival rates, whereas those on the lowest passenger deck, and those whose cabin codes are missing, had the lowest success rate. The only passenger whose cabin was on the Tank Top deck (below the Orlop Deck) did not survive."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.displot(\n","    train_data.sort_values(by=\"Deck\"), x=\"Deck\", hue=\"Survived\", multiple=\"fill\"\n",")\n","plt.ylabel(\"Survival Ratio\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["All missing values have been dealt with."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Feature Engineering"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Family_Size` is created by adding `SibSp, Parch` and 1. Those who travel entirely alone don't seem to have had the highest chance of survival."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Family_Size\"] = data.SibSp + data.Parch + 1\n","\n","sns.countplot(train_data, x=\"Family_Size\", hue=\"Survived\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Ticket_Freq` is created by encoding the frequency with which a ticket occurs, which gives an idea of the size of the group in which one was traveling."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data[\"Ticket_Freq\"] = (\n","    joint_data().groupby(\"Ticket\")[\"Ticket\"].transform(\"count\")[:891]\n",")\n","test_data[\"Ticket_Freq\"] = (\n","    joint_data().groupby(\"Ticket\")[\"Ticket\"].transform(\"count\")[891:]\n",")\n","\n","sns.countplot(train_data, x=\"Ticket_Freq\", hue=\"Survived\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Title` is extracted from the name. All titles with less than 10 samples are joined into a separate category 'Other'. Those with the title Mr. had the lowest survival rate. Mrs. had a higher survival rate than Miss."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Title\"] = (\n","        data[\"Name\"].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n","    )\n","train_data.Title.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.loc[\n","    train_data.groupby(\"Title\")[\"Title\"].transform(\"size\") < 10, \"Title\"\n","] = \"Other\"\n","test_data.loc[~test_data[\"Title\"].isin(train_data[\"Title\"]), \"Title\"] = \"Other\"\n","train_data.Title.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(train_data, x=\"Title\", hue=\"Survived\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# def extract_surname(name):\n","#     if \"(\" in name:\n","#         name_no_bracket = name.split(\"(\")[0]\n","#     else:\n","#         name_no_bracket = name\n","#     family = name_no_bracket.split(\",\")[0]\n","#     return family\n","\n","\n","# for data in [train_data, test_data]:\n","#     data[\"Name\"] = data.Name.apply(extract_surname)\n","\n","# train_data[\"Surname_Freq\"] = (\n","#     joint_data().groupby(\"Name\")[\"Name\"].transform(\"count\")[:891]\n","# )\n","# test_data[\"Surname_Freq\"] = (\n","#     joint_data().groupby(\"Name\")[\"Name\"].transform(\"count\")[891:]\n","# )\n","\n","# sns.countplot(train_data, x=\"Surname_Freq\", hue=\"Survived\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Name` and `Ticket` are dropped as they are no longer needed and contain no useful information."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data.drop(columns=[\"Name\", \"Ticket\"], inplace=True)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["It is worth noting, that some passengers have a `Fare` of 0. Ismay traveled on a complimentary ticket, as well as his servants (Fry and Harrison), Andrews and the Guarantee Group, and Reuchlin. Assuming that the rest of the passengers who hadn't paid for their tickets also received complimentary tickets, a separate 1/0 (True/False) feature `Complimentary` is created to indicate this."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["joint_data()[joint_data().Fare == 0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Complimentary\"] = data.Fare.apply(lambda x: 1 if x == 0 else 0)\n","\n","joint_data()[joint_data().Fare == 0].sample(3)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Fare` is transformed to log scale to deal with right skewness."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.Fare.sort_values().head(50)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Log_Fare\"] = np.log1p(data[\"Fare\"])\n","\n","_, axs = plt.subplots(1, 2, figsize=(12, 5))\n","sns.histplot(\n","    train_data,\n","    x=\"Fare\",\n","    hue=\"Survived\",\n","    binwidth=50,\n","    binrange=(0, 600),\n","    kde=True,\n","    ax=axs[0],\n",")\n","sns.histplot(\n","    train_data,\n","    x=\"Log_Fare\",\n","    hue=\"Survived\",\n","    kde=True,\n","    ax=axs[1],\n",")\n","axs[0].set_title(\"Original\")\n","axs[1].set_title(\"Transformed to Log scale\")\n","\n","for data in [train_data, test_data]:\n","    data.drop(columns=\"Fare\", inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["1 / (train_data.Log_Fare.sort_values().head(50) + 1)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The ratio between the price and quantity could prove to be a useful feature, so it is created by dividing the `Log_Fare` by `Ticket_Freq`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data[\"Log_Fare/Ticket_Freq\"] = data[\"Log_Fare\"] / data[\"Ticket_Freq\"]\n","\n","sns.histplot(\n","    train_data,\n","    x=\"Log_Fare/Ticket_Freq\",\n","    hue=\"Survived\",\n","    kde=True,\n",")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Encoding"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Nominal: `Survived, Sex, Deck, Embarked, Title, Complimentary`  \n","Ordinal: `Pclass`  \n","Continuous: `Age, Log_Fare, Log_Fare/Ticket_Freq`  \n","Discrete: `SibSp, Parch, NCabins, Family_Size, Ticket_Freq` "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Might come in handy\n","nominal = [\"Survived\", \"Sex\", \"Deck\", \"Embarked\", \"Title\", \"Complimentary\"]\n","ordinal = [\"Pclass\"]\n","continuous = [\"Age\", \"Log_Fare\", \"Log_Fare/Ticket_Freq\"]\n","discrete = [\"SibSp\", \"Parch\", \"NCabins\", \"Family_Size\", \"Ticket_Freq\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["joint_data().head()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["All features except `Sex, Deck, Embarked` and `Title` are already properly encoded, so only they are encoded (as a one-hot numeric array). Dummy variables include redundant information, so to overcome the Dummy variable Trap, one dummy per categorical variable is dropped. The choice of which dummy variable to drop is arbitrary and doesn't affect the model's overall performance, so the first is dropped automatically by OneHotEncoder. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feat = [\"Sex\", \"Deck\", \"Embarked\", \"Title\"]\n","ohe = OneHotEncoder(drop=\"first\", sparse_output=False)\n","train_data = train_data.drop(columns=feat).join(ohe.fit_transform(train_data[feat]))\n","test_data = test_data.drop(columns=feat).join(ohe.transform(test_data[feat]))\n","# for data in [train_data, test_data]:\n","#     data.drop(columns=[\"Deck_T\", \"Embarked_Q\"], inplace=True)\n","\n","joint_data().info()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Dealing with Multicollinearity"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SelectCorrelationClusters(BaseEstimator, TransformerMixin):\n","    def __init__(\n","        self,\n","        threshold=0.7,\n","        corr_method=\"spearman\",\n","        linkage_method=\"ward\",\n","        optimal_ordering=True,\n","    ):\n","        self.threshold = threshold\n","        self.corr_method = corr_method\n","        self.linkage_method = linkage_method\n","        self.optimal_ordering = optimal_ordering\n","\n","    def fit(self, X, y=None):\n","        X = pd.DataFrame(X)\n","        # Remove constant columns and calculate correlation\n","        self.constant_cols = np.where(X.nunique() == 1)[0]\n","        self.corr_ = X.drop(columns=X.columns[self.constant_cols]).corr(\n","            method=self.corr_method\n","        )\n","        # Convert the correlation matrix to a distance matrix before performing hierarchical clustering\n","        distance_matrix = 1 - self.corr_.abs()\n","        self.Z = linkage(\n","            squareform(distance_matrix, checks=False),\n","            \"ward\",\n","            optimal_ordering=self.optimal_ordering,\n","        )\n","        # Extract the cluster labels for each feature based on the specified threshold\n","        clusters = fcluster(self.Z, self.threshold, criterion=\"distance\")\n","        # Filter out highly correlated features\n","        # Keep one feature per cluster with the maximum average correlation\n","        self.kept_features = []\n","        for cluster_id in set(clusters):\n","            clusters_boolean = clusters == cluster_id\n","            corr_vals = self.corr_.iloc[clusters_boolean, clusters_boolean]\n","            best_feature_idx = corr_vals.abs().sum(axis=1).idxmax()\n","            self.kept_features.append(corr_vals.columns.get_loc(best_feature_idx))\n","        return self\n","\n","    def transform(self, X, y=None):\n","        X = pd.DataFrame(X)\n","        return X.drop(columns=X.columns[self.constant_cols]).iloc[:, self.kept_features]\n","\n","    def plot(self, annot=True, cmap=\"RdBu\", split=True):\n","        if not split:\n","            # Plot clustermap\n","            sns.clustermap(\n","                self.corr_.round(2),\n","                row_linkage=self.Z,\n","                col_linkage=self.Z,\n","                cmap=cmap,\n","                annot=annot,\n","                annot_kws={\"size\": 8},\n","                vmin=-1,\n","                vmax=1,\n","                figsize=(15, 12),\n","                dendrogram_ratio=0.2,\n","            )\n","        else:\n","            # Plot dendrogram with correlation heatmap\n","            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 8))\n","            dendro = dendrogram(\n","                self.Z,\n","                labels=self.corr_.columns,\n","                ax=ax1,\n","                leaf_rotation=90,\n","            )\n","            order = dendro[\"leaves\"]\n","            sns.heatmap(\n","                self.corr_.iloc[order, order],\n","                cmap=cmap,\n","                annot=annot,\n","                vmin=-1,\n","                vmax=1,\n","                # linewidths=0.01,\n","                # linecolor=\"#23272e\",\n","                ax=ax2,\n","            )\n","            fig.tight_layout()\n","        plt.show()\n","\n","\n","def calculate_vif(X):\n","    # Add constant column\n","    # X = add_constant(X)\n","    X = X.assign(const=1)\n","\n","    # Calculate VIF for each variable\n","    vif = pd.DataFrame()\n","    vif[\"Variable\"] = X.columns\n","    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","    return vif\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["When a dummy variable that represents more than two categories has a high VIF, multicollinearity does not necessarily exist. The variables will always have high VIFs if there is a small portion of cases in the category, regardless of whether the categorical variables are correlated to other variables, so dummy encoded nominal variables are ignored.  \n","`Family_Size, SibSp` and `Parch` have an infinite VIF score and are perfectly multicollinear. They all have high correlation scores, along with `Ticket_Freq`. Considering their MI scores, `Family_Size` is kept and the rest are dropped, along with `Log_Fare`, which also has high correlation scores with those previously mentioned and with `Log_Fare/Ticket_Freq`, with the exception that it also has high correlation with `Pclass` (which the rest do not), which results in a higher VIF score than `Log_Fare/Ticket_Freq`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# impd_age is needed to demonstrate how things work with imputed age values,\n","# which will actually be imputed later, during CV\n","impd_age = age_imputer.fit_transform(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %%capture --no-display\n","display(calculate_vif(impd_age[discrete + continuous + ordinal]))\n","SelectCorrelationClusters().fit(impd_age[discrete + continuous + ordinal]).plot()\n","mutinf = pd.Series(\n","    mutual_info_classif(\n","        impd_age.drop(columns=\"Survived\"),\n","        train_data[\"Survived\"],\n","        discrete_features=[\n","            True\n","            if col\n","            not in [\n","                \"Age\",\n","                \"Log_Fare\",\n","                \"Log_Fare/Ticket_Freq\",\n","            ]\n","            else False\n","            for col in train_data.drop(columns=\"Survived\").columns\n","        ],\n","        # discrete_features=True,\n","        random_state=seed,\n","    ),\n","    index=train_data.drop(columns=\"Survived\").columns,\n",").sort_values(ascending=False)\n","sns.barplot(x=mutinf, y=mutinf.index.astype(str))\n","plt.xlabel(\"Mutual Information score\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for data in [train_data, test_data]:\n","    data.drop(\n","        columns=[\n","            \"SibSp\",\n","            \"Parch\",\n","            \"Ticket_Freq\",\n","            \"Log_Fare\",\n","        ],\n","        inplace=True,\n","    )\n","\n","# Might come in handy\n","nominal = [\"Survived\", \"Sex\", \"Deck\", \"Embarked\", \"Title\", \"Complimentary\"]\n","ordinal = [\"Pclass\"]\n","continuous = [\"Age\", \"Log_Fare/Ticket_Freq\"]\n","discrete = [\"NCabins\", \"Family_Size\"]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now all VIF scores are within reasonable bounds."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["calculate_vif(impd_age[discrete + continuous + ordinal])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Reciprocal"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next the multiplicative inverse of ordinal/continuous/discrete features is added."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ReciprocalTransformer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        # Store thresholds by column name\n","        self.threshold = {}\n","        # Exclude columns that are OHE dummies\n","        self.selected_columns = X.loc[\n","            :, ~np.all(np.logical_or(X == 0, X == 1), axis=0)\n","        ].columns\n","        # Get thresholds\n","        X[self.selected_columns].apply(self.get_thresholds)\n","        return self\n","\n","    def transform(self, X, y=None):\n","        # Apply the transform_zeros function to each selected column\n","        transformed_columns = X[self.selected_columns].apply(self.transform_zeros)\n","        # Take the reciprocal of the transformed columns\n","        reciprocals = 1 / transformed_columns\n","        return X.join(reciprocals, rsuffix=\"_inverse\")\n","\n","    def get_thresholds(self, column):\n","        # Find the minimum positive non-zero value (value closest to zero) in the column\n","        min_nonzero = np.min(column[column != 0].abs())\n","        # Calculate the closest 10^n value below the minimum non-zero value in the column\n","        self.threshold[column.name] = 10 ** np.floor(np.log10(min_nonzero))\n","\n","    def transform_zeros(self, column):\n","        column_values = column.copy()\n","        # Find zero values in the column and replace them with the threshold value\n","        column_values.loc[column_values == 0] = self.threshold[column_values.name]\n","        return column_values\n","\n","\n","ReciprocalTransformer().fit(train_data).transform(train_data).filter(\n","    regex=\"_inverse$\"\n",").head()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Model training and selection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prepare data\n","X = train_data.copy()\n","y = X.pop(\"Survived\")\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=seed, stratify=y\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# SelectFromModel(\n","#     ExtraTreesClassifier(\n","#         random_state=seed,\n","#     ),\n","#     threshold=\"0.01*mean\",\n","# ).fit(\n","#     impd_age, y\n","# ).transform(impd_age)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define score function to go into SelectPercentile\n","discrete_features = [\n","    True\n","    if col\n","    not in [\n","        \"Age\",\n","        \"Log_Fare/Ticket_Freq\",\n","        \"Age_inverted\",\n","        \"Log_Fare/Ticket_Freq_inverted\",\n","    ]\n","    else False\n","    for col in X.columns\n","]\n","\n","\n","def mut_info(X, y):\n","    return mutual_info_classif(\n","        X,\n","        y,\n","        discrete_features=discrete_features,\n","        random_state=seed,\n","    )\n","\n","\n","# Define transformer to clean up after adding feature interactions and standardizing\n","class RemoveDuplicates(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        df = pd.DataFrame(X)\n","        self.not_dup = list(df.columns[~df.T.duplicated()])\n","        return self\n","\n","    def transform(self, X):\n","        df = pd.DataFrame(X)\n","        return df.loc[:, self.not_dup]  # .to_numpy()\n","\n","\n","# Define function to create a DataFrame from top scores\n","def get_top_scores(study, score_count=7):\n","    df = (\n","        study.trials_dataframe(attrs=(\"value\", \"duration\", \"params\"))\n","        .sort_values(by=\"value\", ascending=False)\n","        .head(score_count)\n","    ).rename(\n","        columns=lambda x: re.sub(r\"^params_\", \"\", x)\n","    )  # remove params_ from col names\n","    df[\"duration\"] = df[\"duration\"].dt.total_seconds()\n","    return df\n","\n","\n","# Define function to calculate validation/test scores for top sets of hyperparameters\n","def model_test_score(model, study_name, top_scores, results_storage):\n","    res = top_scores.value.to_list()\n","    for i in range(len(top_scores)):\n","        # Create dictionary of model parameters\n","        model_params = (\n","            top_scores.iloc[i]\n","            .drop(\n","                [\n","                    \"value\",\n","                    \"duration\",\n","                    \"MI_percentile\",\n","                    \"reciprocal\",\n","                    \"selection_l1_C\",\n","                    \"l1_C\",\n","                    \"selection_RF_FI\",\n","                    \"ET_threshold\",\n","                ]\n","            )\n","            .to_dict()\n","        )\n","        # Assign model parameters to estimator\n","        model.set_params(**model_params)\n","        # Create pipeline\n","        pipe = make_pipeline(\n","            age_imputer,\n","            SelectPercentile(mut_info, percentile=top_scores.iloc[i].MI_percentile),\n","            standardize,\n","            cleanup,\n","            model,\n","        )\n","        # Update pipeline with extra steps if needed\n","        if top_scores.iloc[i].reciprocal:\n","            pipe.steps.insert(\n","                2,\n","                (\n","                    \"reciprocal\",\n","                    inverse,\n","                ),\n","            )\n","        if top_scores.iloc[i].selection_l1_C:\n","            pipe.steps.insert(\n","                -1,\n","                (\n","                    \"l1_selection\",\n","                    SelectFromModel(\n","                        LinearSVC(\n","                            C=top_scores.iloc[i].l1_C,\n","                            penalty=\"l1\",\n","                            dual=False,\n","                            max_iter=100000,\n","                            random_state=seed,\n","                        )\n","                    ),\n","                ),\n","            )\n","        if top_scores.iloc[i].selection_RF_FI:\n","            pipe.steps.insert(\n","                -1,\n","                (\n","                    \"ET_selection\",\n","                    SelectFromModel(\n","                        ExtraTreesClassifier(\n","                            random_state=seed,\n","                        ),\n","                        threshold=f\"{top_scores.iloc[i].ET_threshold}*mean\",\n","                    ),\n","                ),\n","            )\n","        # Get and store score results\n","        res.append(pipe.fit(X_train, y_train).score(X_test, y_test))\n","    # Add duration\n","    res.extend(top_scores.duration.to_list())\n","    # Assign results to proper index (model)\n","    results_storage.loc[study_name] = res\n","    # Format for display\n","    df = results_storage.loc[study_name].to_frame().reset_index()\n","    df[[\"col\", \"index\"]] = df[\"index\"].str.split(\"_\", expand=True)\n","    df = df.pivot(index=\"index\", columns=\"col\")\n","    df.columns = df.columns.droplevel()\n","    df.columns.name, df.index.name = None, None\n","    column_order = [\"cv\", \"test\", \"duration\"]\n","    display(df.reindex(column_order, axis=1))\n","\n","\n","# Define function to generate n top submissions\n","def submissions_from_model(\n","    model, study_name, top_scores, results_storage, n_submissions=3\n","):\n","    # Sort parameters by test scores\n","    scores_by_test = top_scores.copy()\n","    scores_by_test[\"sort\"] = model_results.loc[study_name][\n","        len(scores_by_test) : len(scores_by_test) * 2\n","    ].to_numpy()\n","    scores_by_test.sort_values(by=\"sort\", ascending=False, inplace=True)\n","    scores_by_test.drop(columns=\"sort\", inplace=True)\n","\n","    for i in range(n_submissions):\n","        # Create dictionary of model parameters\n","        model_params = (\n","            top_scores.iloc[i]\n","            .drop(\n","                [\n","                    \"value\",\n","                    \"duration\",\n","                    \"MI_percentile\",\n","                    \"reciprocal\",\n","                    \"selection_l1_C\",\n","                    \"l1_C\",\n","                    \"selection_RF_FI\",\n","                    \"ET_threshold\",\n","                ]\n","            )\n","            .to_dict()\n","        )\n","        # Assign model parameters to estimator\n","        model.set_params(**model_params)\n","        # Create pipeline\n","        pipe = make_pipeline(\n","            age_imputer,\n","            SelectPercentile(mut_info, percentile=top_scores.iloc[i].MI_percentile),\n","            standardize,\n","            cleanup,\n","            model,\n","        )\n","        # Update pipeline with extra steps if needed\n","        if top_scores.iloc[i].reciprocal:\n","            pipe.steps.insert(\n","                2,\n","                (\n","                    \"reciprocal\",\n","                    inverse,\n","                ),\n","            )\n","        if top_scores.iloc[i].selection_l1_C:\n","            pipe.steps.insert(\n","                -1,\n","                (\n","                    \"l1_selection\",\n","                    SelectFromModel(\n","                        LinearSVC(\n","                            C=top_scores.iloc[i].l1_C,\n","                            penalty=\"l1\",\n","                            dual=False,\n","                            max_iter=100000,\n","                            random_state=seed,\n","                        )\n","                    ),\n","                ),\n","            )\n","        if top_scores.iloc[i].selection_RF_FI:\n","            pipe.steps.insert(\n","                -1,\n","                (\n","                    \"ET_selection\",\n","                    SelectFromModel(\n","                        ExtraTreesClassifier(\n","                            random_state=seed,\n","                        ),\n","                        threshold=f\"{top_scores.iloc[i].ET_threshold}*mean\",\n","                    ),\n","                ),\n","            )\n","        # Generate predictions\n","        predictions = pipe.fit(X, y).predict(test_data)\n","        # Create submission CSV\n","        predictions_df = pd.DataFrame(\n","            {\"PassengerId\": PassengerId, \"Survived\": predictions}\n","        )\n","        predictions_df.to_csv(\n","            f\"{study_name}_submission_{i+1}.csv\", header=True, index=False\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize pipeline steps\n","inverse = ReciprocalTransformer()\n","standardize = StandardScaler()\n","cleanup = RemoveDuplicates()\n","\n","# Define DB file name for storing tuning results\n","db_name = \"titanic_study\"\n","storage_name = f\"sqlite:///{db_name}.db\"\n","\n","# Create DataFrame to store results of model evaluations\n","score_count = 7\n","model_results = pd.DataFrame(\n","    columns=[f\"cv_{i}\" for i in range(1, score_count + 1)]\n","    + [f\"test_{i}\" for i in range(1, score_count + 1)]\n","    + [f\"duration_{i}\" for i in range(1, score_count + 1)]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Turn off optuna log notes.\n","optuna.logging.set_verbosity(optuna.logging.WARN)\n","\n","\n","# Define a function to output a log only when the best value is updated\n","def logging_callback(study, frozen_trial):\n","    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n","    if previous_best_value != study.best_value:\n","        study.set_user_attr(\"previous_best_value\", study.best_value)\n","        print(\n","            \"Trial {} finished with best value: {} and parameters: {}. \".format(\n","                frozen_trial.number,\n","                frozen_trial.value,\n","                frozen_trial.params,\n","            )\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the objective of the study without the model\n","def cv_pipe(trial, model):\n","    # Create Pipeline\n","    pipe = make_pipeline(\n","        age_imputer,\n","        SelectPercentile(\n","            mut_info, percentile=trial.suggest_int(\"MI_percentile\", 1, 100)\n","        ),\n","        standardize,\n","        cleanup,\n","        # perform SelectFromModel from RF feature importances? what threshold?\n","        model,\n","    )\n","    if trial.suggest_categorical(\"reciprocal\", [True, False]):\n","        pipe.steps.insert(\n","            2,\n","            (\n","                \"reciprocal\",\n","                inverse,\n","            ),\n","        )\n","    if trial.suggest_categorical(\"selection_l1_C\", [True, False]):\n","        pipe.steps.insert(\n","            -1,\n","            (\n","                \"l1_selection\",\n","                SelectFromModel(\n","                    LinearSVC(\n","                        C=trial.suggest_float(\"l1_C\", 1e-2, 20, log=True),\n","                        penalty=\"l1\",\n","                        dual=False,\n","                        max_iter=100000,\n","                        random_state=seed,\n","                    )\n","                ),\n","            ),\n","        )\n","    if trial.suggest_categorical(\"selection_RF_FI\", [True, False]):\n","        pipe.steps.insert(\n","            -1,\n","            (\n","                \"ET_selection\",\n","                SelectFromModel(\n","                    ExtraTreesClassifier(\n","                        random_state=seed,\n","                    ),\n","                    threshold=f\"{trial.suggest_float('ET_threshold', 1e-2, 1)}*mean\",\n","                ),\n","            ),\n","        )\n","    # Calculate scoring metric\n","    cv_score = cross_val_score(\n","        pipe,\n","        X_train,\n","        y_train,\n","        cv=RepeatedStratifiedKFold(n_splits=2, n_repeats=1, random_state=seed),\n","        n_jobs=-1,\n","    ).mean()\n","    return cv_score\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize model\n","model = LogisticRegression(\n","    max_iter=100000,\n","    n_jobs=-1,\n","    random_state=rng,\n",")\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial):\n","    # Set estimator parameters\n","    model.set_params(\n","        C=trial.suggest_float(\"C\", 1e-4, 1000, log=True),\n","        solver=trial.suggest_categorical(\n","            \"solver\", [\"liblinear\", \"lbfgs\", \"newton-cg\", \"sag\", \"saga\"]\n","        ),\n","    )\n","    return cv_pipe(trial, model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define unique identifier of the study\n","study_name = \"Logistic_Regression\"\n","\n","study = optuna.create_study(\n","    storage=storage_name,\n","    study_name=study_name,\n","    sampler=optuna.samplers.TPESampler(\n","        multivariate=True, seed=seed, warn_independent_sampling=False\n","    ),\n","    direction=\"maximize\",\n","    load_if_exists=True,\n",")\n","print(f\"Sampler: {study.sampler.__class__.__name__}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Perform hyperparameter optimization search\n","# study.optimize(\n","#     objective_w_model,\n","#     n_trials=30,\n","#     timeout=300,\n","#     # catch=(ValueError),\n","#     # callbacks=[logging_callback],\n","# )\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter optimization search\n","study.optimize(\n","    objective_w_model,\n","    n_trials=700,\n","    timeout=3600,\n","    catch=(ValueError),\n","    # callbacks=[logging_callback],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_scores = get_top_scores(study)\n","top_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_test_score(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_from_model(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_slice(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_edf(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(\n","    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### K-Nearest Neighbours"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = KNeighborsClassifier(\n","    n_jobs=-1,\n",")\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial):\n","    # Set estimator parameters\n","    model.set_params(n_neighbors=trial.suggest_int(\"n_neighbors\", 4, 40))\n","    return cv_pipe(trial, model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define unique identifier of the study\n","study_name = \"K-Nearest_Neighbors\"\n","\n","study = optuna.create_study(\n","    storage=storage_name,\n","    study_name=study_name,\n","    sampler=optuna.samplers.TPESampler(seed=seed),\n","    direction=\"maximize\",\n","    load_if_exists=True,\n",")\n","print(f\"Sampler: {study.sampler.__class__.__name__}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter optimization search\n","study.optimize(\n","    objective_w_model,\n","    n_trials=700,\n","    timeout=3600,\n","    catch=(ValueError),\n","    # callbacks=[logging_callback],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_scores = get_top_scores(study)\n","top_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_test_score(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_from_model(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_slice(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_edf(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(\n","    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = RandomForestClassifier(n_jobs=-1, random_state=rng)\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial):\n","    # Set estimator parameters\n","    model.set_params(\n","        max_depth=trial.suggest_int(\"max_depth\", 5, 50),\n","        min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 3, 150),\n","        max_features=trial.suggest_float(\"max_features\", 0.2, 0.8),\n","        ccp_alpha=trial.suggest_float(\"ccp_alpha\", 1e-9, 0.1),\n","    )\n","    return cv_pipe(trial, model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define unique identifier of the study\n","study_name = \"Random_Forest\"\n","\n","study = optuna.create_study(\n","    storage=storage_name,\n","    study_name=study_name,\n","    sampler=optuna.samplers.TPESampler(seed=seed),\n","    direction=\"maximize\",\n","    load_if_exists=True,\n",")\n","print(f\"Sampler: {study.sampler.__class__.__name__}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter optimization search\n","study.optimize(\n","    objective_w_model,\n","    n_trials=700,\n","    timeout=3600,\n","    catch=(ValueError),\n","    # callbacks=[logging_callback],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_scores = get_top_scores(study)\n","top_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_test_score(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_from_model(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_slice(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_edf(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(\n","    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Support Vector Machine"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = SVC(gamma=\"scale\", max_iter=100000, random_state=rng, cache_size=1000)\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial):\n","    # Set estimator parameters\n","    model.set_params(\n","        C=trial.suggest_float(\"C\", 1e-4, 1000, log=True),\n","        kernel=trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"sigmoid\"]),\n","    )\n","    return cv_pipe(trial, model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define unique identifier of the study\n","study_name = \"Support Vector Machine\"\n","\n","study = optuna.create_study(\n","    storage=storage_name,\n","    study_name=study_name,\n","    sampler=optuna.samplers.TPESampler(seed=seed),\n","    direction=\"maximize\",\n","    load_if_exists=True,\n",")\n","print(f\"Sampler: {study.sampler.__class__.__name__}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter optimization search\n","study.optimize(\n","    objective_w_model,\n","    n_trials=700,\n","    timeout=3600,\n","    catch=(ValueError),\n","    # callbacks=[logging_callback],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_scores = get_top_scores(study)\n","top_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_test_score(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_from_model(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_slice(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_edf(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(\n","    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### AdaBoost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = AdaBoostClassifier(estimator=DecisionTreeClassifier(), random_state=rng)\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial):\n","    # Set estimator parameters\n","    model.set_params(\n","        estimator__max_depth=trial.suggest_int(\"estimator__max_depth\", 1, 3),\n","        n_estimators=trial.suggest_int(\"n_estimators\", 10, 100),\n","        learning_rate=trial.suggest_float(\"learning_rate\", 1e-7, 1, log=True),\n","    )\n","    return cv_pipe(trial, model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define unique identifier of the study\n","study_name = \"AdaBoost\"\n","\n","study = optuna.create_study(\n","    storage=storage_name,\n","    study_name=study_name,\n","    sampler=optuna.samplers.TPESampler(seed=seed),\n","    direction=\"maximize\",\n","    load_if_exists=True,\n",")\n","print(f\"Sampler: {study.sampler.__class__.__name__}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter optimization search\n","study.optimize(\n","    objective_w_model,\n","    n_trials=700,\n","    timeout=3600,\n","    catch=(ValueError),\n","    # callbacks=[logging_callback],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_scores = get_top_scores(study)\n","top_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_test_score(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_from_model(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_slice(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_edf(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(\n","    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Gradient Boosting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = GradientBoostingClassifier(\n","    random_state=rng,\n",")\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial):\n","    # Set estimator parameters\n","    model.set_params(\n","        n_estimators=trial.suggest_int(\"n_estimators\", 30, 400),\n","        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1, log=True),\n","        subsample=trial.suggest_float(\"subsample\", 0.1, 1),\n","        min_samples_split=trial.suggest_float(\"min_samples_leaf\", 1e-3, 1e-1, log=True),\n","        max_depth=trial.suggest_int(\"max_depth\", 2, 30),\n","        max_features=trial.suggest_float(\"max_features\", 0.1, 0.9),\n","        ccp_alpha=trial.suggest_float(\"ccp_alpha\", 1e-9, 0.1),\n","    )\n","    return cv_pipe(trial, model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define unique identifier of the study\n","study_name = \"Gradient Boosting\"\n","\n","study = optuna.create_study(\n","    storage=storage_name,\n","    study_name=study_name,\n","    sampler=optuna.samplers.TPESampler(seed=seed),\n","    direction=\"maximize\",\n","    load_if_exists=True,\n",")\n","print(f\"Sampler: {study.sampler.__class__.__name__}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter optimization search\n","study.optimize(\n","    objective_w_model,\n","    n_trials=700,\n","    timeout=3600,\n","    catch=(ValueError),\n","    # callbacks=[logging_callback],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_scores = get_top_scores(study)\n","top_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_test_score(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_from_model(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_slice(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_edf(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(\n","    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize estimator\n","model = XGBClassifier(booster=\"gbtree\", random_state=rng, verbosity=0)\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial):\n","    # Set estimator parameters\n","    model.set_params(\n","        n_estimators=trial.suggest_int(\"n_estimators\", 30, 500),\n","        eta=trial.suggest_float(\"eta\", 1e-4, 3, log=True),\n","        subsample=trial.suggest_float(\"subsmaple\", 0.1, 1),\n","        max_depth=trial.suggest_int(\"max_depth\", 2, 30),\n","        min_child_weight=trial.suggest_float(\"min_child_weight\", 0, 50),\n","        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1),\n","        scale_pos_weight=trial.suggest_float(\"scale_pos_weight\", 1e-4, 2),\n","        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 10),\n","        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 10),\n","    )\n","    return cv_pipe(trial, model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define unique identifier of the study\n","study_name = \"XGBoost\"\n","\n","study = optuna.create_study(\n","    storage=storage_name,\n","    study_name=study_name,\n","    sampler=optuna.samplers.TPESampler(seed=seed),\n","    direction=\"maximize\",\n","    load_if_exists=True,\n",")\n","print(f\"Sampler: {study.sampler.__class__.__name__}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter optimization search\n","study.optimize(\n","    objective_w_model,\n","    n_trials=700,\n","    timeout=3600,\n","    catch=(ValueError),\n","    # callbacks=[logging_callback],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_scores = get_top_scores(study)\n","top_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_test_score(model, study_name, top_scores, model_results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_from_model(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_slice(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_edf(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(\n","    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_results[\"mean_cv_score\"] = model_results.filter(regex=r\"^cv_\").mean(axis=1)\n","model_results[\"std_cv_score\"] = model_results.filter(regex=r\"^cv_\").std(axis=1)\n","model_results[\"mean_test_score\"] = model_results.filter(regex=r\"^test_\").mean(axis=1)\n","model_results[\"std_test_score\"] = model_results.filter(regex=r\"^test_\").std(axis=1)\n","model_results[\"mean_duration\"] = model_results.filter(regex=r\"^duration_\").mean(axis=1)\n","model_results.drop(\n","    columns=model_results.filter(regex=r\"^duration_\").columns, inplace=True\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# mean_diff = []\n","# for i in range(1, len(model_results) + 1):\n","#     mean_diff.append(\n","#         (\n","#             model_results.filter(regex=r\"^test_\").T.reset_index().iloc[:, i]\n","#             - model_results.filter(regex=r\"^cv_\").T.reset_index().iloc[:, i]\n","#         ).mean()\n","#     )\n","# model_results[\"mean_diff\"] = mean_diff"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_results.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the objective of the study without the model\n","def cv_pipe(trial, model):\n","    # Create Pipeline\n","    pipe = make_pipeline(\n","        age_imputer,\n","        SelectPercentile(\n","            mut_info, percentile=trial.suggest_int(\"MI_percentile\", 1, 100)\n","        ),\n","        standardize,\n","        cleanup,\n","        # SelectFromPI(percentile=trial.suggest_int(\"perm_perc\", 0, 100)),\n","        # PCA(n_components=trial.suggest_float('PCA_components', 1e-2, 1)),  # float? variance explained is greater than percentage\n","        model,\n","    )\n","    if trial.suggest_categorical(\"reciprocal\", [True, False]):\n","        pipe.steps.insert(\n","            2,\n","            (\n","                \"reciprocal\",\n","                inverse,\n","            ),\n","        )\n","    if trial.suggest_categorical(\"selection_l1_C\", [True, False]):\n","        pipe.steps.insert(\n","            -1,\n","            (\n","                \"l1_selection\",\n","                SelectFromModel(\n","                    LinearSVC(\n","                        C=trial.suggest_float(\"l1_C\", 1e-2, 20, log=True),\n","                        penalty=\"l1\",\n","                        dual=False,\n","                        max_iter=100000,\n","                        random_state=seed,\n","                    )\n","                ),\n","            ),\n","        )\n","    # if trial.suggest_categorical(\"selection_RF_FI\", [True, False]):\n","    #     pipe.steps.insert(\n","    #         -1,\n","    #         (\n","    #             \"ET_selection\",\n","    #             SelectFromModel(\n","    #                 ExtraTreesClassifier(\n","    #                     random_state=seed,\n","    #                 ),\n","    #                 threshold=f\"{trial.suggest_float('ET_threshold', 1e-2, 1)}*mean\",\n","    #             ),\n","    #         ),\n","    #     )\n","    # Calculate scoring metric\n","    cv_score = cross_val_score(\n","        pipe,\n","        X_train,\n","        y_train,\n","        cv=RepeatedStratifiedKFold(n_splits=7, n_repeats=2, random_state=seed),\n","        n_jobs=-1,\n","    ).mean()\n","    return cv_score\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rng = np.random.RandomState(seed)\n","# Initialize model\n","model = LogisticRegression(\n","    max_iter=100000,\n","    n_jobs=-1,\n","    random_state=rng,\n",")\n","\n","\n","# Define objective with the model\n","def objective_w_model(trial):\n","    # Set estimator parameters\n","    model.set_params(\n","        C=trial.suggest_float(\"C\", 1e-4, 1000, log=True),\n","        solver=trial.suggest_categorical(\n","            \"solver\", [\"liblinear\", \"lbfgs\", \"newton-cg\", \"sag\", \"saga\"]\n","        ),\n","    )\n","    return cv_pipe(trial, model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define unique identifier of the study\n","study_name = \"Logistic_Regression\"\n","\n","study = optuna.create_study(\n","    storage=storage_name,\n","    study_name=study_name,\n","    sampler=optuna.samplers.TPESampler(multivariate=True, seed=seed),\n","    direction=\"maximize\",\n","    load_if_exists=True,\n",")\n","print(f\"Sampler: {study.sampler.__class__.__name__}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter optimization search\n","study.optimize(\n","    objective_w_model,\n","    n_trials=700,\n","    timeout=3600,\n","    catch=(ValueError),\n","    # callbacks=[logging_callback],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_scores = get_top_scores(study)\n","top_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_test_score(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_from_model(model, study_name, top_scores, model_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# optuna.visualization.plot_parallel_coordinate(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# optuna.visualization.plot_contour(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_slice(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_edf(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(study)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optuna.visualization.plot_param_importances(\n","    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SelectFromPI:\n","    def __init__(self, percentile):\n","        self.percentile = percentile\n","        self.selector = SelectPercentile(\n","            score_func=self.permutation_importance_from_model,\n","            percentile=self.percentile,\n","        )\n","\n","    @staticmethod\n","    def permutation_importance_from_model(X, y):\n","        model.fit(X, y)\n","        return permutation_importance(model, X, y, random_state=seed).importances_mean\n","\n","    def fit(self, X, y):\n","        self.selector.fit(X, y)\n","\n","        # Check if no features selected\n","        # if self.selector.get_support().sum() == 0:\n","        # Select the best feature based on the provided score function\n","        # best_feature_idx = self.selector.scores_.argmax()\n","        # print(self.selector.scores_)\n","        # print(best_feature_idx)\n","        # self.selector.get_support()[best_feature_idx] = True\n","        return self\n","\n","    def transform(self, X):\n","        # Check if no features selected\n","        if self.selector.get_support().sum() == 0:\n","            # Select the best feature based on the provided score function\n","            best_feature_idx = self.selector.scores_.argmax()\n","            return pd.DataFrame(X).iloc[:, [best_feature_idx]]\n","        return self.selector.transform(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["perm_rf = permutation_importance(\n","    rf_fit, X, y, random_state=seed, n_repeats=3, n_jobs=-1\n",")\n","perm_pipe = permutation_importance(\n","    pipe_fit, X, y, random_state=seed, n_repeats=3, n_jobs=-1\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["perm_imp = (\n","    pd.DataFrame(perm_pipe.importances, index=X.columns)\n","    .reset_index()\n","    .melt(id_vars=\"index\")\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.barplot(\n","    perm_imp,\n","    x=\"value\",\n","    y=\"index\",\n","    order=perm_imp.groupby(\"index\")[\"value\"].mean().sort_values(ascending=False).index,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.barplot(\n","    x=perm_pipe.importances_mean,\n","    y=X.columns,\n","    order=[\n","        item[1]\n","        for item in sorted(zip(perm_pipe.importances_mean, X.columns), reverse=True)\n","    ],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_key = sorted(zip(randf.feature_importances_, randf.feature_names_in_), reverse=True)\n","val_key = pd.DataFrame(val_key)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.barplot(x=val_key[0], y=val_key[1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_key = sorted(zip(model.feature_importances_), reverse=True)\n","val_key = pd.DataFrame(val_key)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# val_key\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.barplot(x=val_key[0], y=np.arange(val_key.shape[0]).astype(str))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.n_features_in_\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
